"""
Autonomous Exploitation Agent with Self-Learning Capabilities.

Advanced agent that uses LLM-powered reasoning for:
- Autonomous vulnerability discovery through iterative hypothesis testing
- Self-learning from successful/failed exploitation attempts
- Dynamic decision-making for exploit strategy selection
- Multi-step exploitation chain planning
- Knowledge retention across assessments

Similar to advanced AI agents (Claude-like), this agent reasons about
vulnerabilities, makes autonomous decisions, and learns from experience.
"""

from __future__ import annotations

import json
import time
from typing import Any, Dict, List, Optional
from dataclasses import dataclass, field

from cortexsec.agents.real_world_guidance import real_world_prompt
from cortexsec.core.agent import BaseAgent, Finding, PentestContext


@dataclass
class ExploitHypothesis:
    """Represents a hypothesis about a potential vulnerability."""
    
    vulnerability_type: str
    target_component: str  
    hypothesis: str
    confidence: float
    test_strategy: str
    expected_evidence: str
    priority: int = 5


@dataclass
class ExploitAttempt:
    """Records an exploitation attempt and its outcome."""
    
    hypothesis: ExploitHypothesis
    action_taken: str
    outcome: str  # success, partial, failed
    evidence_collected: str
    timestamp: float
    lessons_learned: str
    confidence_delta: float


@dataclass
class LearningMemory:
    """Long-term memory for self-learning."""
    
    successful_patterns: List[Dict[str, Any]] = field(default_factory=list)
    failed_patterns: List[Dict[str, Any]] = field(default_factory=list)
    vulnerability_knowledge: Dict[str, Any] = field(default_factory=dict)
    exploitation_stats: Dict[str, int] = field(default_factory=lambda: {
        "total_attempts": 0,
        "successful": 0,
        "partial": 0,
        "failed": 0
    })


class AutonomousExploitationAgent(BaseAgent):
    """
    Advanced autonomous agent with self-learning and decision-making.
    
    This agent operates with Claude-like reasoning capabilities:
    - Generates hypotheses about vulnerabilities autonomously
    - Makes strategic decisions about exploitation approaches
    - Learns from successes and failures
    - Adapts strategy based on accumulated knowledge
    """

    def __init__(self, llm, memory_path: str = "reports/exploit_memory.json", max_iterations: int = 5):
        super().__init__("AutonomousExploitationAgent", llm)
        self.memory_path = memory_path
        self.max_iterations = max_iterations
        self.learning_memory = self._load_memory()

    def _load_memory(self) -> LearningMemory:
        """Load long-term learning memory."""
        import os
        
        if not os.path.exists(self.memory_path):
            return LearningMemory()
        
        try:
            with open(self.memory_path, "r") as f:
                data = json.load(f)
                return LearningMemory(**data)
        except Exception:
            return LearningMemory()

    def _save_memory(self):
        """Persist learning memory."""
        import os
        
        directory = os.path.dirname(self.memory_path)
        if directory:
            os.makedirs(directory, exist_ok=True)
        
        with open(self.memory_path, "w") as f:
            json.dump({
                "successful_patterns": self.learning_memory.successful_patterns,
                "failed_patterns": self.learning_memory.failed_patterns,
                "vulnerability_knowledge": self.learning_memory.vulnerability_knowledge,
                "exploitation_stats": self.learning_memory.exploitation_stats
            }, f, indent=2)

    def _generate_hypotheses(self, context: PentestContext) -> List[ExploitHypothesis]:
        """
        Use LLM to autonomously generate vulnerability hypotheses.
        
        This mimics Claude-like reasoning by asking the LLM to think about
        potential vulnerabilities based on the gathered intelligence.
        """
        # Prepare context for LLM reasoning
        intelligence_summary = {
            "target": context.target,
            "attack_surface": context.attack_surface,
            "existing_findings": [
                {
                    "title": f.title,
                    "severity": f.severity,
                    "confidence": f.confidence,
                    "evidence": f.evidence[:200]  # Truncate for context
                }
                for f in context.findings[:10]  # Top 10 findings
            ],
            "recon_data": {
                "technologies": context.recon_data.get("analysis", {}).get("technologies", []),
                "exposed_services": context.attack_surface.get("exposed_services", [])
            },
            "past_successful_patterns": self.learning_memory.successful_patterns[-5:],  # Last 5 successes
        }

        reasoning_prompt = f"""
        You are an elite penetration testing AI with autonomous reasoning capabilities.
        
        Analyze the following target intelligence and generate hypotheses about potential vulnerabilities
        that could be exploited. Think step-by-step like an expert security researcher.
        
        INTELLIGENCE:
        {json.dumps(intelligence_summary, indent=2)}
        
        Based on this intelligence and your knowledge of security vulnerabilities, generate 3-5 hypotheses
        about potential exploitable vulnerabilities. For each hypothesis:
        
        1. Think about what vulnerability type could exist
        2. Consider which component is most likely vulnerable
        3. Formulate a testable hypothesis
        4. Assess your confidence (0.0-1.0)
        5. Design a safe test strategy
        6. Define expected evidence
        7. Assign priority (1-10, 10 being highest)
        
        Return JSON:
        {{
            "reasoning": "Your step-by-step thought process",
            "hypotheses": [
                {{
                    "vulnerability_type": "e.g., SQL Injection, XSS, SSRF",
                    "target_component": "e.g., login endpoint, API parameter",
                    "hypothesis": "Detailed hypothesis statement",
                    "confidence": 0.75,
                    "test_strategy": "Specific safe test approach",
                    "expected_evidence": "What successful exploitation would show",
                    "priority": 8
                }}
            ]
        }}
        """

        try:
            response = self.llm.generate_json(
                reasoning_prompt,
                system_prompt=real_world_prompt("autonomous security researcher with advanced reasoning capabilities")
            )

            hypotheses = []
            for h_data in response.get("hypotheses", [])[:5]:  # Max 5 hypotheses
                hypotheses.append(ExploitHypothesis(
                    vulnerability_type=h_data.get("vulnerability_type", "Unknown"),
                    target_component=h_data.get("target_component", "Unknown"),
                    hypothesis=h_data.get("hypothesis", ""),
                    confidence=float(h_data.get("confidence", 0.5)),
                    test_strategy=h_data.get("test_strategy", ""),
                    expected_evidence=h_data.get("expected_evidence", ""),
                    priority=int(h_data.get("priority", 5))
                ))

            # Log LLM reasoning
            self.log(f"LLM Reasoning: {response.get('reasoning', 'N/A')[:200]}...")
            
            return sorted(hypotheses, key=lambda h: (h.priority, h.confidence), reverse=True)

        except Exception as e:
            self.log(f"Failed to generate hypotheses: {e}")
            return []

    def _decide_exploitation_strategy(self, hypothesis: ExploitHypothesis, context: PentestContext) -> Dict[str, Any]:
        """
        Use LLM to make autonomous decision about exploitation approach.
        
        This demonstrates Claude-like decision-making.
        """
        decision_prompt = f"""
        You are making an autonomous decision about how to test this vulnerability hypothesis.
        
        HYPOTHESIS:
        - Type: {hypothesis.vulnerability_type}
        - Component: {hypothesis.target_component}
        - Statement: {hypothesis.hypothesis}
        - Confidence: {hypothesis.confidence}
        - Proposed Strategy: {hypothesis.test_strategy}
        
        TARGET: {context.target}
        MODE: {context.mode}
        
        Based on this information and ethical hacking principles, decide:
        1. Should this hypothesis be tested? (yes/no/maybe)
        2. What specific action should be taken? (detailed, safe, non-destructive)
        3. What are the risks?
        4. What safety measures are needed?
        5. What success criteria should we use?
        
        Return JSON:
        {{
            "decision": "yes/no/maybe",
            "reasoning": "Why you made this decision",
            "action": "Specific safe action to take",
            "risks": ["list", "of", "risks"],
            "safety_measures": ["list", "of", "safety", "checks"],
            "success_criteria": "How to know if successful"
        }}
        """

        try:
            decision = self.llm.generate_json(
                decision_prompt,
                system_prompt=real_world_prompt("autonomous decision-making security agent")
            )
            return decision
        except Exception as e:
            self.log(f"Decision-making failed: {e}")
            return {"decision": "no", "reasoning": "LLM decision failed"}

    def _execute_safe_test(self, hypothesis: ExploitHypothesis, strategy: Dict[str, Any], context: PentestContext) -> ExploitAttempt:
        """
        Execute a safe, non-destructive test based on LLM decision.
        
        IMPORTANT: This never executes destructive actions, only safe validation.
        """
        attempt = ExploitAttempt(
            hypothesis=hypothesis,
            action_taken=strategy.get("action", "No action"),
            outcome="not_executed",
            evidence_collected="",
            timestamp=time.time(),
            lessons_learned="",
            confidence_delta=0.0
        )

        # Safety check: Only execute in authorized mode
        if context.mode != "authorized":
            attempt.outcome = "blocked"
            attempt.lessons_learned = "Mode not authorized for autonomous testing"
            return attempt

        # For safety, we'll simulate the test and use LLM to reason about outcomes
        # In a real implementation, you could integrate with existing agents
        
        simulation_prompt = f"""
        Simulate the outcome of this safe security test:
        
        ACTION: {strategy.get('action', 'Unknown')}
        HYPOTHESIS: {hypothesis.hypothesis}
        TARGET: {context.target}
        
        Based on the existing findings and intelligence, reason about what the outcome
        would likely be if this test were executed safely.
        
        Return JSON:
        {{
            "outcome": "success/partial/failed",
            "evidence": "What evidence would be collected",
            "lessons_learned": "What we learned from this",
            "confidence_change": -0.2 to +0.3
        }}
        """

        try:
            result = self.llm.generate_json(simulation_prompt)
            
            attempt.outcome = result.get("outcome", "failed")
            attempt.evidence_collected = result.get("evidence", "No evidence")
            attempt.lessons_learned = result.get("lessons_learned", "")
            attempt.confidence_delta = float(result.get("confidence_change", 0.0))
            
        except Exception as e:
            self.log(f"Test simulation failed: {e}")
            attempt.outcome = "failed"
            attempt.lessons_learned = f"Execution error: {str(e)}"

        return attempt

    def _learn_from_attempt(self, attempt: ExploitAttempt):
        """Update self-learning memory based on attempt outcome."""
        self.learning_memory.exploitation_stats["total_attempts"] += 1

        if attempt.outcome == "success":
            self.learning_memory.exploitation_stats["successful"] += 1
            
            # Store successful pattern
            pattern = {
                "vulnerability_type": attempt.hypothesis.vulnerability_type,
                "test_strategy": attempt.hypothesis.test_strategy,
                "evidence_pattern": attempt.evidence_collected,
                "confidence": attempt.hypothesis.confidence,
                "timestamp": attempt.timestamp
            }
            self.learning_memory.successful_patterns.append(pattern)
            
            # Keep only last 50 successful patterns
            self.learning_memory.successful_patterns = self.learning_memory.successful_patterns[-50:]

        elif attempt.outcome == "partial":
            self.learning_memory.exploitation_stats["partial"] += 1

        elif attempt.outcome == "failed":
            self.learning_memory.exploitation_stats["failed"] += 1
            
            # Store failed pattern to avoid repeating
            pattern = {
                "vulnerability_type": attempt.hypothesis.vulnerability_type,
                "test_strategy": attempt.hypothesis.test_strategy,
                "reason": attempt.lessons_learned,
                "timestamp": attempt.timestamp
            }
            self.learning_memory.failed_patterns.append(pattern)
            
            # Keep only last 100 failed patterns
            self.learning_memory.failed_patterns = self.learning_memory.failed_patterns[-100:]

        # Update vulnerability knowledge
        vuln_type = attempt.hypothesis.vulnerability_type
        if vuln_type not in self.learning_memory.vulnerability_knowledge:
            self.learning_memory.vulnerability_knowledge[vuln_type] = {
                "attempts": 0,
                "successes": 0,
                "success_rate": 0.0
            }

        self.learning_memory.vulnerability_knowledge[vuln_type]["attempts"] += 1
        if attempt.outcome == "success":
            self.learning_memory.vulnerability_knowledge[vuln_type]["successes"] += 1

        # Calculate success rate
        attempts = self.learning_memory.vulnerability_knowledge[vuln_type]["attempts"]
        successes = self.learning_memory.vulnerability_knowledge[vuln_type]["successes"]
        self.learning_memory.vulnerability_knowledge[vuln_type]["success_rate"] = round(successes / attempts, 3)

    def run(self, context: PentestContext) -> PentestContext:
        """
        Execute autonomous exploitation with self-learning.
        
        This agent operates autonomously:
        1. Generate vulnerability hypotheses using LLM reasoning
        2. Make decisions about testing strategy
        3. Execute safe tests
        4. Learn from outcomes
        5. Iterate and improve
        """
        self.log("Starting autonomous exploitation agent with self-learning capabilities...")

        all_attempts = []
        new_findings = []

        for iteration in range(self.max_iterations):
            self.log(f"Autonomous iteration {iteration + 1}/{self.max_iterations}")

            # Step 1: Generate hypotheses autonomously
            hypotheses = self._generate_hypotheses(context)
            self.log(f"Generated {len(hypotheses)} hypotheses")

            if not hypotheses:
                self.log("No additional hypotheses generated. Stopping.")
                break

            # Step 2: Process top hypothesis
            for hypothesis in hypotheses[:2]:  # Top 2 per iteration
                self.log(f"Testing: {hypothesis.vulnerability_type} in {hypothesis.target_component}")

                # Step 3: Make autonomous decision
                strategy = self._decide_exploitation_strategy(hypothesis, context)

                if strategy.get("decision") != "yes":
                    self.log(f"Decision: {strategy.get('decision')} - {strategy.get('reasoning', 'N/A')}")
                    continue

                # Step 4: Execute safe test
                attempt = self._execute_safe_test(hypothesis, strategy, context)
                all_attempts.append(attempt)

                # Step 5: Learn from attempt
                self._learn_from_attempt(attempt)

                # Step 6: Create finding if successful
                if attempt.outcome in {"success", "partial"}:
                    finding = Finding(
                        title=f"[Autonomous] {hypothesis.vulnerability_type} in {hypothesis.target_component}",
                        description=f"{hypothesis.hypothesis}\n\nDiscovered through autonomous reasoning and testing.",
                        severity="High" if attempt.outcome == "success" else "Medium",
                        confidence=min(hypothesis.confidence + attempt.confidence_delta, 1.0),
                        evidence=f"Autonomous test result: {attempt.evidence_collected}\n\nStrategy: {strategy.get('action', 'N/A')}",
                        mitigation=f"Address the identified {hypothesis.vulnerability_type} vulnerability. {attempt.lessons_learned}",
                        cvss_score=7.5 if attempt.outcome == "success" else 5.5,
                        owasp_mapping="A03:2021 - Injection" if "injection" in hypothesis.vulnerability_type.lower() else "A01:2021 - Broken Access Control",
                        mitre_mapping="T1190 - Exploit Public-Facing Application"
                    )
                    new_findings.append(finding)
                    context.findings.append(finding)

        # Save learning memory
        self._save_memory()

        # Add to context history
        context.history.append({
            "agent": self.name,
            "message": "Autonomous exploitation complete",
            "iterations": len(all_attempts),
            "successful_tests": len([a for a in all_attempts if a.outcome == "success"]),
            "new_findings": len(new_findings),
            "learning_stats": self.learning_memory.exploitation_stats,
            "knowledge_areas": len(self.learning_memory.vulnerability_knowledge)
        })

        self.log(f"Autonomous agent complete. {len(new_findings)} new findings from {len(all_attempts)} attempts.")
        self.log(f"Success rate: {self.learning_memory.exploitation_stats['successful']}/{self.learning_memory.exploitation_stats['total_attempts']}")

        return context
