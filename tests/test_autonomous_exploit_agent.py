"""Tests for Autonomous Exploitation Agent."""

import json
import os
import tempfile

import pytest

from cortexsec.agents.autonomous_exploit_agent import (
    AutonomousExploitationAgent,
    ExploitHypothesis,
    LearningMemory,
)
from cortexsec.core.agent import PentestContext


class DummyLLM:
    def __init__(self, json_responses=None):
        self.json_responses = json_responses or []
        self.call_count = 0

    def generate_json(self, prompt: str, system_prompt: str = ""):
        if self.call_count < len(self.json_responses):
            response = self.json_responses[self.call_count]
            self.call_count += 1
            return response
        return {}


def base_context():
    return PentestContext(
        target="https://example.com",
        mode="authorized",
        recon_data={
            "raw": {"server": "nginx"},
            "analysis": {"technologies": ["nginx", "php"]},
        },
        attack_surface={"exposed_services": ["http", "https"]},
    )


def test_autonomous_agent_initialization():
    """Test agent initializes with learning memory."""
    with tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".json") as f:
        memory_path = f.name

    try:
        agent = AutonomousExploitationAgent(DummyLLM(), memory_path=memory_path)
        
        assert agent.name == "AutonomousExploitationAgent"
        assert isinstance(agent.learning_memory, LearningMemory)
        assert agent.learning_memory.exploitation_stats["total_attempts"] == 0
    finally:
        if os.path.exists(memory_path):
            os.unlink(memory_path)


def test_hypothesis_generation():
    """Test LLM-powered hypothesis generation."""
    llm = DummyLLM(json_responses=[
        {
            "reasoning": "Based on nginx and PHP, SQL injection is likely",
            "hypotheses": [
                {
                    "vulnerability_type": "SQL Injection",
                    "target_component": "login endpoint",
                    "hypothesis": "Login form may be vulnerable to SQL injection",
                    "confidence": 0.8,
                    "test_strategy": "Test with safe SQL payloads",
                    "expected_evidence": "Database error or delayed response",
                    "priority": 9
                },
                {
                    "vulnerability_type": "XSS",
                    "target_component": "search parameter",
                    "hypothesis": "Search input may reflect without encoding",
                    "confidence": 0.7,
                    "test_strategy": "Test with benign script tags",
                    "expected_evidence": "Script execution in response",
                    "priority": 7
                }
            ]
        }
    ])

    with tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".json") as f:
        memory_path = f.name

    try:
        agent = AutonomousExploitationAgent(llm, memory_path=memory_path)
        context = base_context()

        hypotheses = agent._generate_hypotheses(context)

        assert len(hypotheses) == 2
        assert hypotheses[0].vulnerability_type == "SQL Injection"
        assert hypotheses[0].priority == 9
        assert hypotheses[0].confidence == 0.8
        # Should be sorted by priority then confidence
        assert hypotheses[0].priority >= hypotheses[1].priority
    finally:
        if os.path.exists(memory_path):
            os.unlink(memory_path)


def test_autonomous_decision_making():
    """Test LLM-powered decision making."""
    llm = DummyLLM(json_responses=[
        {
            "decision": "yes",
            "reasoning": "Safe to test with non-destructive payload",
            "action": "Send safe SQL test payload to login endpoint",
            "risks": ["False positive", "Rate limiting"],
            "safety_measures": ["Use test account", "Monitor for errors"],
            "success_criteria": "Database error or timing anomaly"
        }
    ])

    with tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".json") as f:
        memory_path = f.name

    try:
        agent = AutonomousExploitationAgent(llm, memory_path=memory_path)
        context = base_context()

        hypothesis = ExploitHypothesis(
            vulnerability_type="SQL Injection",
            target_component="login",
            hypothesis="Login vulnerable to SQLi",
            confidence=0.8,
            test_strategy="Safe SQL payloads",
            expected_evidence="Error message",
            priority=9
        )

        decision = agent._decide_exploitation_strategy(hypothesis, context)

        assert decision["decision"] == "yes"
        assert "safe" in decision["reasoning"].lower()
        assert len(decision["risks"]) > 0
        assert len(decision["safety_measures"]) > 0
    finally:
        if os.path.exists(memory_path):
            os.unlink(memory_path)


def test_self_learning_from_success():
    """Test that agent learns from successful attempts."""
    llm = DummyLLM()

    with tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".json") as f:
        memory_path = f.name

    try:
        agent = AutonomousExploitationAgent(llm, memory_path=memory_path)

        hypothesis = ExploitHypothesis(
            vulnerability_type="XSS",
            target_component="search",
            hypothesis="Reflected XSS",
            confidence=0.75,
            test_strategy="Test script tags",
            expected_evidence="Script execution",
            priority=8
        )

        from cortexsec.agents.autonomous_exploit_agent import ExploitAttempt
        import time

        attempt = ExploitAttempt(
            hypothesis=hypothesis,
            action_taken="Tested safe XSS payload",
            outcome="success",
            evidence_collected="Script executed in response",
            timestamp=time.time(),
            lessons_learned="XSS confirmed in search parameter",
            confidence_delta=0.2
        )

        # Learn from attempt
        agent._learn_from_attempt(attempt)

        assert agent.learning_memory.exploitation_stats["total_attempts"] == 1
        assert agent.learning_memory.exploitation_stats["successful"] == 1
        assert len(agent.learning_memory.successful_patterns) == 1
        assert agent.learning_memory.successful_patterns[0]["vulnerability_type"] == "XSS"
        
        # Check vulnerability knowledge updated
        assert "XSS" in agent.learning_memory.vulnerability_knowledge
        assert agent.learning_memory.vulnerability_knowledge["XSS"]["successes"] == 1
        assert agent.learning_memory.vulnerability_knowledge["XSS"]["success_rate"] == 1.0

    finally:
        if os.path.exists(memory_path):
            os.unlink(memory_path)


def test_self_learning_from_failure():
    """Test that agent learns from failed attempts."""
    llm = DummyLLM()

    with tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".json") as f:
        memory_path = f.name

    try:
        agent = AutonomousExploitationAgent(llm, memory_path=memory_path)

        hypothesis = ExploitHypothesis(
            vulnerability_type="SSRF",
            target_component="url parameter",
            hypothesis="SSRF in URL fetch",
            confidence=0.6,
            test_strategy="Test internal IP",
            expected_evidence="Internal response",
            priority=7
        )

        from cortexsec.agents.autonomous_exploit_agent import ExploitAttempt
        import time

        attempt = ExploitAttempt(
            hypothesis=hypothesis,
            action_taken="Tested SSRF payload",
            outcome="failed",
            evidence_collected="No internal access",
            timestamp=time.time(),
            lessons_learned="URL parameter is validated",
            confidence_delta=-0.1
        )

        agent._learn_from_attempt(attempt)

        assert agent.learning_memory.exploitation_stats["total_attempts"] == 1
        assert agent.learning_memory.exploitation_stats["failed"] == 1
        assert len(agent.learning_memory.failed_patterns) == 1
        assert agent.learning_memory.failed_patterns[0]["vulnerability_type"] == "SSRF"

    finally:
        if os.path.exists(memory_path):
            os.unlink(memory_path)


def test_memory_persistence():
    """Test that learning memory persists across sessions."""
    with tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".json") as f:
        memory_path = f.name

    try:
        # First session: create agent and learn something
        agent1 = AutonomousExploitationAgent(DummyLLM(), memory_path=memory_path)
        
        hypothesis = ExploitHypothesis(
            vulnerability_type="Command Injection",
            target_component="exec endpoint",
            hypothesis="Command injection",
            confidence=0.9,
            test_strategy="Safe command test",
            expected_evidence="Command output",
            priority=10
        )

        from cortexsec.agents.autonomous_exploit_agent import ExploitAttempt
        import time

        attempt = ExploitAttempt(
            hypothesis=hypothesis,
            action_taken="Tested command injection",
            outcome="success",
            evidence_collected="Command executed",
            timestamp=time.time(),
            lessons_learned="Exec endpoint is vulnerable",
            confidence_delta=0.1
        )

        agent1._learn_from_attempt(attempt)
        agent1._save_memory()

        # Second session: load memory
        agent2 = AutonomousExploitationAgent(DummyLLM(), memory_path=memory_path)

        # Memory should be loaded
        assert agent2.learning_memory.exploitation_stats["total_attempts"] == 1
        assert agent2.learning_memory.exploitation_stats["successful"] == 1
        assert len(agent2.learning_memory.successful_patterns) == 1
        assert "Command Injection" in agent2.learning_memory.vulnerability_knowledge

    finally:
        if os.path.exists(memory_path):
            os.unlink(memory_path)


def test_full_autonomous_run():
    """Test full autonomous exploitation run."""
    llm = DummyLLM(json_responses=[
        # Hypothesis generation
        {
            "reasoning": "Testing autonomous agent",
            "hypotheses": [
                {
                    "vulnerability_type": "Path Traversal",
                    "target_component": "file download",
                    "hypothesis": "File download vulnerable to path traversal",
                    "confidence": 0.85,
                    "test_strategy": "Test with ../ sequences",
                    "expected_evidence": "Unauthorized file access",
                    "priority": 9
                }
            ]
        },
        # Decision making
        {
            "decision": "yes",
            "reasoning": "Safe to test",
            "action": "Test path traversal",
            "risks": ["False positive"],
            "safety_measures": ["Non-destructive test"],
            "success_criteria": "File contents leaked"
        },
        # Test simulation
        {
            "outcome": "success",
            "evidence": "Successfully accessed /etc/passwd",
            "lessons_learned": "Path traversal confirmed",
            "confidence_change": 0.15
        }
    ])

    with tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".json") as f:
        memory_path = f.name

    try:
        agent = AutonomousExploitationAgent(llm, memory_path=memory_path, max_iterations=1)
        context = base_context()

        result_context = agent.run(context)

        # Should have generated findings
        assert len(result_context.findings) > 0
        
        # Check history
        assert any("Autonomous exploitation complete" in h.get("message", "") for h in result_context.history)
        
        # Learning stats should be updated
        assert agent.learning_memory.exploitation_stats["total_attempts"] > 0

    finally:
        if os.path.exists(memory_path):
            os.unlink(memory_path)


def test_unauthorized_mode_blocking():
    """Test that agent blocks in unauthorized mode."""
    llm = DummyLLM(json_responses=[
        {
            "reasoning": "Test",
            "hypotheses": [
                {
                    "vulnerability_type": "Test",
                    "target_component": "test",
                    "hypothesis": "test",
                    "confidence": 0.8,
                    "test_strategy": "test",
                    "expected_evidence": "test",
                    "priority": 5
                }
            ]
        },
        {
            "decision": "yes",
            "reasoning": "Test",
            "action": "test",
            "risks": [],
            "safety_measures": [],
            "success_criteria": "test"
        }
    ])

    with tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".json") as f:
        memory_path = f.name

    try:
        agent = AutonomousExploitationAgent(llm, memory_path=memory_path, max_iterations=1)
        context = base_context()
        context.mode = "lab"  # Not authorized

        result_context = agent.run(context)

        # Should not generate findings in lab mode
        # (since test execution is blocked)
        assert len(result_context.findings) == 0

    finally:
        if os.path.exists(memory_path):
            os.unlink(memory_path)
